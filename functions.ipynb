{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e222d495-5b2d-4653-b2e3-f52c184e547d",
   "metadata": {},
   "source": [
    "# Data to embeddings\n",
    "\n",
    "Steps :\n",
    "1. Get the pdf file path\n",
    "2. Extract the text\n",
    "3. Convert the text pages into sentences\n",
    "4. Chunk the sentences\n",
    "5. Convert into embeddings with the help of embeddings models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9aee9b0-dce9-4489-8264-d45067741953",
   "metadata": {},
   "source": [
    "## Download the pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2876ec0d-25b6-4888-800f-dc4a1a72ebd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to download the pdf\n",
    "\n",
    "import os\n",
    "import requests\n",
    "\n",
    "def download_pdf(url : str,\n",
    "                 filepath : str = \"pdf/\",\n",
    "                 filename : str = \"data.pdf\",\n",
    "                 verbose : bool = False):\n",
    "    \"\"\"download_pdf() is used to download the pdf file using the url\"\"\"\n",
    "    file = filepath + filename\n",
    "\n",
    "    # if there is no folder so we will create a folder\n",
    "    if not os.path.exists(filepath):\n",
    "        os.makedirs(filepath)\n",
    "        if verbose :\n",
    "            print(f\"[INFO] {filepath} is created successfully\")\n",
    "\n",
    "    if verbose :\n",
    "        print(f\"[INFO] {file} is downloading ...\")\n",
    "\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        with open(file,\"wb\") as file:\n",
    "            file.write(response.content)\n",
    "\n",
    "        if verbose :\n",
    "            print(f\"[INFO] {file} is saved ...\")\n",
    "    else:\n",
    "        print(f\"[WARNING] Failed to get the pdf from the url : {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f13047ae-b2de-43ba-99a1-cb17d57086ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] tests/ is created successfully\n",
      "[INFO] tests/human-nutrition-text.pdf is downloading ...\n",
      "[INFO] <_io.BufferedWriter name='tests/human-nutrition-text.pdf'> is saved ...\n"
     ]
    }
   ],
   "source": [
    "filename = \"human-nutrition-text.pdf\"\n",
    "filepath = \"tests/\"\n",
    "url = \"https://pressbooks.oer.hawaii.edu/humannutrition2/open/download?type=pdf\"\n",
    "download_pdf(url,filepath,filename,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedb879d-25c5-44c9-ac2d-4fe067e920f6",
   "metadata": {},
   "source": [
    "## Convert the pdf into dict\n",
    "Parameters:\n",
    "1. Page number\n",
    "2. Sentence_chunk\n",
    "3. Character count\n",
    "4. Word count\n",
    "5. Token count\n",
    "6. Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c67f77c-c4dc-422a-9d44-2cc337ae32c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "import fitz\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "def text_formatter(text : str) -> str:\n",
    "    \"\"\" Convert the text that contains the /n with the space\"\"\"\n",
    "    formatted_text = text.replace('\\n',' ').strip()\n",
    "    \n",
    "    return formatted_text\n",
    "\n",
    "def count_and_split_sentence(text : str) -> (int,list[str]):\n",
    "    \"\"\"To count and split the sentences from the given text \"\"\"\n",
    "    nlp = English()\n",
    "    nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "    list_of_sentences = list(nlp(text).sents)\n",
    "    list_of_sentences = [str(sentence) for sentence in list_of_sentences]\n",
    "\n",
    "    return len(list_of_sentences),list_of_sentences\n",
    "\n",
    "def open_pdf(filename : str,\n",
    "             starting_page_number : int = 0) -> list[dict]:\n",
    "    \"\"\"convert the pdf into dict dtype\"\"\"\n",
    "\n",
    "    doc = fitz.open(filename)\n",
    "    data = []\n",
    "\n",
    "    print(\"[INFO] Converting the pdf into dict dtype\")\n",
    "    for page_number,page in tqdm(enumerate(doc)):\n",
    "        text = page.get_text()\n",
    "        text = text_formatter(text = text)\n",
    "\n",
    "        sentence_count,sentences = count_and_split_sentence(text)\n",
    "\n",
    "        data.append(\n",
    "            {\n",
    "                \"page_number\" : page_number - starting_page_number,\n",
    "                \"char_count\" : len(text),\n",
    "                \"word_count\" : len(text.split(\" \")),\n",
    "                \"sentence_count\" : sentence_count,\n",
    "                \"token_count\" : len(text) / 4,\n",
    "                \"sentence\" : sentences,\n",
    "                \"text\" : text\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2897b8a4-9a8d-4630-b7d4-4d15143c8ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_the_array(array_list : list,\n",
    "                    chunk_length : int) -> list[list[str]]:\n",
    "    \"\"\"Split the array of sentences into groups of chunks\"\"\"\n",
    "    return [array_list[i:i+chunk_length] for i in range(0,len(array_list),chunk_length)]\n",
    "\n",
    "def convert_to_chunk(filepath : str = \"pdf/\",\n",
    "                     filename : str = \"data.pdf\",\n",
    "                     starting_page_number : int = 0,\n",
    "                     chunk_size : int = 10) -> list[dict]:\n",
    "    \"\"\" Convert the sentences into chunks \"\"\"\n",
    "    pages_and_texts = open_pdf(filepath+filename,starting_page_number)\n",
    "    pages_and_chunks = []\n",
    "\n",
    "    # splitting the chunks \n",
    "    print(\"[INFO] Splitting the sentences \")\n",
    "    for item in tqdm(pages_and_texts):\n",
    "        item[\"sentence_chunks\"] = split_the_array(item[\"sentence\"],chunk_size)\n",
    "        item[\"chunk_count\"] = len(item[\"sentence_chunks\"])\n",
    "\n",
    "    # splitting the chunks\n",
    "    print(\"[INFO] Splitting into chunks \")\n",
    "    for item in tqdm(pages_and_texts):\n",
    "        for chunks in item[\"sentence_chunks\"]:\n",
    "            d = {}\n",
    "            d[\"page_number\"] = item[\"page_number\"]\n",
    "            \n",
    "            joined_sentence = \"\".join(chunks).replace(\"  \",\" \").strip()\n",
    "            joined_sentence = re.sub(r'\\.([A-Z])', r'. \\1',joined_sentence) # .A -> . A it is used to provide a space after a sentence ends\n",
    "\n",
    "            d[\"sentence_chunk\"] = joined_sentence\n",
    "            # stats\n",
    "            d[\"char_count\"] = len(joined_sentence)\n",
    "            d[\"word_count\"] = len(list(joined_sentence.split(\" \")))\n",
    "            d[\"token_count\"] = len(joined_sentence) / 4 # 4 tokens ~ 1 word\n",
    "\n",
    "            pages_and_chunks.append(d)\n",
    "\n",
    "    return pages_and_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ad777c-2041-44f6-a466-e44e83e56b4b",
   "metadata": {},
   "source": [
    "## Convert the chunks into embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "374d8d1e-ae13-4ee8-a183-bd816bdcc52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda import is_available\n",
    "from torch import device\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedding_model = \"all-mpnet-base-v2\"\n",
    "device = device('cuda' if is_available() else 'cpu')\n",
    "\n",
    "def convert_to_embedds(model_name : str,\n",
    "                       device : str,\n",
    "                       filepath : str = \"pdf/\",\n",
    "                       filename : str = \"data.pdf\",\n",
    "                       starting_page_number : int = 0,\n",
    "                       chunk_size = 10) -> list[dict] :\n",
    "    \n",
    "    data = convert_to_chunk(filepath,\n",
    "                        filename,\n",
    "                        starting_page_number,\n",
    "                        chunk_size)\n",
    "    \n",
    "    embedding_model = SentenceTransformer(model_name_or_path = model_name,device = device)\n",
    "    print(\"[INFO] Converting into embeddings \")\n",
    "    for item in tqdm(data):\n",
    "        item[\"embeddings\"] = embedding_model.encode(item[\"sentence_chunk\"], convert_to_tensor = True)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5dc59d59-3dad-4f77-85d9-91080617132a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Converting the pdf into dict dtype\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1208it [04:17,  4.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Splitting the sentences \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1208/1208 [00:00<00:00, 528609.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Splitting into chunks \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1208/1208 [00:00<00:00, 24636.03it/s]\n",
      "E:\\jai\\apps\\miniconda\\envs\\ChatWithPDF\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "E:\\jai\\apps\\miniconda\\envs\\ChatWithPDF\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Converting into embeddings \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1843/1843 [14:29<00:00,  2.12it/s]\n"
     ]
    }
   ],
   "source": [
    "data = convert_to_embedds(embedding_model,device,\"tests/\",\"human-nutrition-text.pdf\",0,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6412c1ba-dd2e-4254-a80d-a5e91fa26314",
   "metadata": {},
   "source": [
    "## Save the dict in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a88d2433-7982-4f2f-a7bf-440432fc236e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def save_the_embeddings(filename : str = \"embeddings.csv\",\n",
    "                        filepath : str = \"pdf/\",\n",
    "                       data : list[dict] = None,\n",
    "                       verbose = False,\n",
    "                       embedding_model : str = \"all-mpnet-base-v2\",\n",
    "                       device :str = 'cpu'):\n",
    "    embedd_file = filepath + filename\n",
    "    if data is None:\n",
    "        data = convert_to_embedds(embedding_model,device,\"tests/\",\"human-nutrition-text.pdf\",0,10)\n",
    "    dataframe = pd.DataFrame(data)\n",
    "    dataframe.to_csv(embedd_file,index = False)\n",
    "\n",
    "    if verbose :\n",
    "        print(f\"[INFO] {embedd_file} is successfully saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0fd3f801-b3bf-4886-993a-80ca3ddac93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] tests/embeddings.csv is successfully saved\n"
     ]
    }
   ],
   "source": [
    "save_the_embeddings(filename = \"embeddings.csv\",filepath=\"tests/\",data=data,verbose = True,embedding_model = \"all-mpnet-base-v2\",device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50d5ba9-d446-4a3f-b432-89cd4442ead0",
   "metadata": {},
   "source": [
    "# Retrieval to generation\n",
    "Steps:\n",
    "1. Get the embeddings in tensor\n",
    "2. Do similarity search\n",
    "3. Initialize the LLM\n",
    "4. Prompt\n",
    "5. Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31b19037-bab7-4b97-8eb0-7e7bfe1e9846",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\jai\\apps\\miniconda\\envs\\ChatWithPDF\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer,util\n",
    "\n",
    "\n",
    "user_query = \"processed foods\"\n",
    "model_name = \"all-mpnet-base-v2\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "data_filename = \"tests/embeddings.csv\"\n",
    "data_pd = pd.read_csv(data_filename)\n",
    "data_dict = pd.read_csv(\"tests/embeddings.csv\").to_dict(orient='records')\n",
    "\n",
    "\n",
    "embedding_model = SentenceTransformer(model_name_or_path = \"all-mpnet-base-v2\", device = device)\n",
    "\n",
    "\n",
    "def get_embeddings(data,device) -> list:\n",
    "    \"\"\"Returns the embeddings from the csv file\"\"\"\n",
    "    data_embeddings = []\n",
    "\n",
    "    for tensor_str in data_pd[\"embeddings\"]:\n",
    "        values_str = tensor_str.split(\"[\")[1].split(\"]\")[0]\n",
    "        values_list = [float(val) for val in values_str.split(\",\")]\n",
    "        tensor_result = torch.tensor(values_list)\n",
    "        data_embeddings.append(tensor_result)\n",
    "\n",
    "    data_embeddings = torch.stack(data_embeddings).to(device)\n",
    "    return data_embeddings\n",
    "\n",
    "\n",
    "def wrap_the_text(text : str,wrap_length : int = 100):\n",
    "    \"\"\"wrap_the_text prettify the text\"\"\"\n",
    "    wrapped_text = textwrap.fill(text,wrap_length)\n",
    "    print(wrapped_text)\n",
    "    \n",
    "def print_retrieved_text(query : str,\n",
    "                         score,\n",
    "                         index : int,\n",
    "                         pages_and_chunks = data_dict):\n",
    "    \"\"\"Prints the retrieved texts\"\"\"\n",
    "    print(f\"Query : {query}\")\n",
    "    for score, index in zip(score,index):\n",
    "        print(f\"Score : {score}\")\n",
    "        print(\"The Text :\")\n",
    "        print(f\"Index : {index}\")\n",
    "        wrap_the_text(pages_and_chunks[index][\"sentence_chunk\"])\n",
    "\n",
    "def display_the_page(index : int , \n",
    "                     starting_page_count : int,\n",
    "                     filename : str = \"human-nutrition-text.pdf\",\n",
    "                     filepath : str = \"tests/\"):\n",
    "    \"\"\"Displays the content of the pdf in Image\"\"\"\n",
    "    doc = fitz.open(filepath + filename)\n",
    "    page = doc.load_page(index + starting_page_count) \n",
    "\n",
    "    # convert page text into np.array\n",
    "    img = page.get_pixmap(dpi=300)\n",
    "    doc.close()\n",
    "    img_array = np.frombuffer(img.samples_mv,dtype=np.uint8).reshape((img.h, img.w, img.n))\n",
    "\n",
    "    # display the image by matplotlib\n",
    "    plt.imshow(img_array)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "def retrieve_relevant_resource( user_query : str ,\n",
    "                               embeddings, \n",
    "                               embedding_model, \n",
    "                               device,\n",
    "                               k = 5,\n",
    "                               to_print : bool = False,\n",
    "                               to_display : bool = False):\n",
    "    \"\"\"Function to retrieve relevant resource\"\"\"\n",
    "    \n",
    "    query_embedding = embedding_model.encode(user_query, convert_to_tensor = True).to(device)\n",
    "\n",
    "    dot_score = util.dot_score( a = query_embedding, b = embeddings)[0]\n",
    "    score , idx = torch.topk(dot_score,k=k)\n",
    "\n",
    "    if to_print:\n",
    "        print_retrieved_text(user_query,score,idx)\n",
    "\n",
    "    if to_display:\n",
    "        for scores,index in zip(score,idx):\n",
    "            print(f\"Score : {scores}\")\n",
    "            print(f\"Page number : {data_dict[index]['page_number']}\")\n",
    "            try :\n",
    "                display_the_page(data_dict[index]['page_number'],41)\n",
    "            except ValueError:\n",
    "                pass\n",
    "            \n",
    "    return score,idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1613f87-9b9f-4d53-baf2-0e9679cde414",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89b630cf-ebd0-4b0e-9a6b-c4d007f129a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query : processed foods\n",
      "Score : 0.6357370615005493\n",
      "The Text :\n",
      "Index : 1603\n",
      "creating products that have a much longer shelf life than raw foods. Also, food processing protects\n",
      "the health of the consumer and allows for easier shipment and the marketing of foods by\n",
      "corporations. However, there are certain drawbacks. Food processing can reduce the nutritional\n",
      "content of raw ingredients. For example, canning involves the use of heat, which destroys the\n",
      "vitamin C in canned fruit. Also, certain food additives that are included during processing, such as\n",
      "high fructose corn syrup, can affect the health of a consumer. However, the level of added sugar can\n",
      "make a major difference. Small amounts of added sugar and other sweeteners, about 6 to 9 teaspoons a\n",
      "day or less, are not considered harmful.1 Food Additives If you examine the label for a processed\n",
      "food product, it is not unusual to see a long list of added materials. These natural or synthetic\n",
      "substances are food additives and there are more than three hundred used during food processing\n",
      "today. The most popular additives are benzoates, nitrites, sulfites, and sorbates, which prevent\n",
      "molds and yeast from growing on food.2 Food additives 1.\n",
      "Score : 0.5948609113693237\n",
      "The Text :\n",
      "Index : 1602\n",
      "Image by Dean Hochman / CC BY 2.0 Food Processing UNIVERSITY OF HAWAI‘I AT MĀNOA FOOD SCIENCE AND\n",
      "HUMAN NUTRITION PROGRAM AND HUMAN NUTRITION PROGRAM Food processing includes the methods and\n",
      "techniques used to transform raw ingredients into packaged food. Workers in this industry use\n",
      "harvested crops or slaughtered and butchered livestock to create products that are marketed to the\n",
      "public. There are different ways in which food can be processed, from a one-off product, such as a\n",
      "wedding cake, to a mass-produced product, such as a line of cupcakes packaged and sold in stores.\n",
      "The Pros and Cons of Food Processing Food processing has a number of important benefits, such as\n",
      "Food Processing | 1025\n",
      "Score : 0.577634871006012\n",
      "The Text :\n",
      "Index : 1592\n",
      "transforming raw ingredients into packaged food, from fresh-baked goods to frozen dinners. Although\n",
      "there are numerous benefits to both, preservation and processing also pose some concerns, in terms\n",
      "of both nutrition and sustainability. Learning Activities Technology Note: The second edition of the\n",
      "Human Nutrition Open Educational Resource (OER) textbook features interactive learning activities. \n",
      "These activities are available in the web-based textbook and not available in the downloadable\n",
      "versions (EPUB, Digital PDF, Print_PDF, or Open Document). Learning activities may be used across\n",
      "various mobile devices, however, for the best user experience it is strongly recommended that users\n",
      "complete these activities using a desktop or laptop computer and in Google Chrome.   An interactive\n",
      "or media element has been excluded from this version of the text. You can view it online here:\n",
      "http:/ /pressbooks.oer.hawaii.edu/ humannutrition2/?p=532   1018 | The Food System\n",
      "Score : 0.5538948774337769\n",
      "The Text :\n",
      "Index : 427\n",
      "States consider this—in the United States approximately 130 million adults are overweight, and 30\n",
      "percent of them are considered obese. The obesity epidemic has reached young adults and children and\n",
      "will markedly affect the prevalence of serious health consequences in adulthood. Health consequences\n",
      "linked to being overweight or obese include Type 2 diabetes, cardiovascular disease, arthritis,\n",
      "depression, and some cancers. An infatuation with sugary foods and refined grains likely contributes\n",
      "to the epidemic proportion of people who are overweight or obese in this country, but so do the\n",
      "consumption of high-calorie foods that contain too much saturated fat and the sedentary lifestyle of\n",
      "most Americans. There is much disagreement over whether high-carbohydrate diets increase weight-gain\n",
      "and disease risk, especially when calories are not significantly higher between compared diets. Many\n",
      "scientific studies demonstrate positive correlations between diets high in added sugars with weight\n",
      "gain and disease risk, but some others do not show a significant relationship. In regard to refined\n",
      "grains, there are no studies that show consumption of refined grains increases weight gain or\n",
      "disease risk. What is clear, however, is that getting more of your carbohydrates from dietary\n",
      "sources containing whole grains instead of refined grains stimulates weight loss and reduces disease\n",
      "risk. A major source of added sugars in the American diet is soft drinks. There is consistent\n",
      "scientific evidence that consuming sugary soft drinks increases weight gain and disease risk.\n",
      "Score : 0.5416259765625\n",
      "The Text :\n",
      "Index : 1654\n",
      "butter on your toast, making your own salad dressing using olive oil, vinegar or lemon juice, and\n",
      "herbs, cooking with olive oil exclusively, or simply adding a dose of it to your favorite meal.11\n",
      "The Raw Food Diet The raw food diet is followed by those who avoid cooking as much as possible in\n",
      "order to take advantage of the full nutrient content of foods. The principle behind raw foodism is\n",
      "that plant foods in their natural state are the most wholesome for the body. The raw food diet is\n",
      "not a weight-loss plan, it is a lifestyle choice. People who practice raw foodism eat only uncooked\n",
      "and unprocessed foods, emphasizing whole fruits and vegetables. Staples of the raw food diet include\n",
      "whole grains, beans, dried fruits, seeds and nuts, seaweed, sprouts, and unprocessed produce. As a\n",
      "result, food preparation mostly involves peeling, chopping, blending, straining, and dehydrating\n",
      "fruits and vegetables. The positive aspects of this eating method include consuming foods that are\n",
      "high in fiber and nutrients, and low in calories and saturated fat. However, the raw food diet\n",
      "offers little in the way of protein, dairy, or fats, which can cause deficiencies of the vitamins A,\n",
      "D, E, and K. In addition, not all foods are healthier uncooked, such as spinach and tomatoes. Also,\n",
      "cooking eliminates potentially 11. More Olive Oil in Diet Could Cut Stroke Risk: Study.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([0.6357, 0.5949, 0.5776, 0.5539, 0.5416]),\n",
       " tensor([1603, 1602, 1592,  427, 1654]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_embeddings = get_embeddings(data_pd,device)\n",
    "\n",
    "retrieve_relevant_resource(\n",
    "    user_query,\n",
    "    data_embeddings, \n",
    "    embedding_model, \n",
    "    device,\n",
    "    k = 5,\n",
    "    to_print = True,\n",
    "    to_display = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fac5f53b-6c8a-4aee-8412-e897f16d15d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer , AutoModelForCausalLM\n",
    "\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# LLM\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_id,\n",
    "                                                 torch_dtype=torch.float16).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9ee7430-af02-40e5-b272-20706aeb8ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Parameters Count : 1100048384\n"
     ]
    }
   ],
   "source": [
    "def get_model_num_params(model: torch.nn.Module):\n",
    "    return sum([param.numel() for param in model.parameters()])\n",
    "\n",
    "print(f\" Parameters Count : {get_model_num_params(llm_model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "323f5d32-b68d-42ab-ad0d-9594c86636b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_mem_bytes': 2211633920, 'model_mem_mb': 2109.18, 'model_mem_gb': 2.06}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_model_mem_size(model: torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Get how much memory a PyTorch model takes up.\n",
    "    \"\"\"\n",
    "    # Get model parameters and buffer sizes\n",
    "    mem_params = sum([param.nelement() * param.element_size() for param in model.parameters()])\n",
    "    mem_buffers = sum([buf.nelement() * buf.element_size() for buf in model.buffers()])\n",
    "\n",
    "    # Calculate various model sizes\n",
    "    model_mem_bytes = mem_params + mem_buffers # in bytes\n",
    "    model_mem_mb = model_mem_bytes / (1024**2) # in megabytes\n",
    "    model_mem_gb = model_mem_bytes / (1024**3) # in gigabytes\n",
    "\n",
    "    return {\"model_mem_bytes\": model_mem_bytes,\n",
    "            \"model_mem_mb\": round(model_mem_mb, 2),\n",
    "            \"model_mem_gb\": round(model_mem_gb, 2)}\n",
    "\n",
    "get_model_mem_size(llm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c4da64-695c-45e9-bd6c-c29350cac297",
   "metadata": {},
   "source": [
    "## Prompt Engg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f79589dc-6e32-4067-818f-ab41ad149c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_formatter(query: str, \n",
    "                     context_items: list[dict]) -> str:\n",
    "    \"\"\"\n",
    "    Augments query with text-based context from context_items.\n",
    "    \"\"\"\n",
    "    # Join context items into one dotted paragraph\n",
    "    context = \"- \" + \"\\n- \".join([item[\"sentence_chunk\"] for item in context_items])\n",
    "\n",
    "    base_prompt = \"\"\"Based on the following context items, please answer the query.\n",
    "Use the following example as reference for the ideal answer style.\n",
    "\\nExample :\n",
    "Query: What are the fat-soluble vitamins?\n",
    "Answer: The fat-soluble vitamins include Vitamin A, Vitamin D, Vitamin E, and Vitamin K. These vitamins are absorbed along with fats in the diet and can be stored in the body's fatty tissue and liver for later use. Vitamin A is important for vision, immune function, and skin health. Vitamin D plays a critical role in calcium absorption and bone health. Vitamin E acts as an antioxidant, protecting cells from damage. Vitamin K is essential for blood clotting and bone metabolism.\n",
    "\\nNow use the following context items to answer the user query:\n",
    "{context}\n",
    "\\nRelevant passages: <extract relevant passages from the context here>\n",
    "User query: {query}\n",
    "Answer:\"\"\"\n",
    "\n",
    "    # Update base prompt with context items and query   \n",
    "    base_prompt = base_prompt.format(context=context, query=query)\n",
    "\n",
    "    # Create prompt template for instruction-tuned model\n",
    "    dialogue_template = [\n",
    "        {\"role\": \"user\",\n",
    "        \"content\": base_prompt}\n",
    "    ]\n",
    "\n",
    "    # Apply the chat template\n",
    "    prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n",
    "                                          tokenize=False,\n",
    "                                          add_generation_prompt=True)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7021d8ca-dd0b-4e03-8788-8770f3927fb5",
   "metadata": {},
   "source": [
    "## Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8292d26e-2f8a-4a3d-814f-c608e90ab4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_text = \"What is malnutrition?\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    # Get relevant resources\n",
    "scores, indices = retrieve_relevant_resource(user_text,data_embeddings, embedding_model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7be79743-0f9a-48d4-b8da-8df010e66a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of context items\n",
    "context_items = [data_dict[i] for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e500bd1-2945-4e6e-8c97-f12a14f72c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format prompt with context items\n",
    "prompt = prompt_formatter(query=user_text,\n",
    "                              context_items=context_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a7678cf-dff1-4897-8759-d69d726a9bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652e6595-3259-4b6e-95d8-9f35a35a8eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Generate an output of tokens\n",
    "outputs = llm_model.generate(**input_ids,max_new_tokens=256) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740284d9-0c11-4425-ac8d-eb3a43cb1f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the output tokens into text\n",
    "output_text = tokenizer.decode(outputs[0])\n",
    "output_text = output_text.split(\"<|assistant|>\")\n",
    "\n",
    "output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1bce9d-aa7f-47f8-b28d-42b91e696771",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(user_text : str):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    scores, indices = retrieve_relevant_resource(user_text,data_embeddings, embedding_model, device)\n",
    "    context_items = [data_dict[i] for i in indices]\n",
    "    prompt = prompt_formatter(query=user_text,context_items=context_items)\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = llm_model.generate(**input_ids,max_new_tokens=256) \n",
    "    output_text = tokenizer.decode(outputs[0])\n",
    "    output_text = output_text.split(\"<|assistant|>\")\n",
    "    \n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bfdf08-0248-497b-84be-a609d58a3182",
   "metadata": {},
   "source": [
    "# Gradio app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9553ee0f-9eca-41ac-9490-a4ea922d835d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
